{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis With TensorFlow and LSTM Network\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "#### Data\n",
    "- using [IMDb Movie Review dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) containing 25K movie reviews (12.5K positive / 12.5K negative)\n",
    "\n",
    "#### Input Representation\n",
    "- using lexicon of 400,000 word vectors provided by [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- map each movie review to it's corresponding list of word vectors\n",
    "\n",
    "#### Recurrent Neural Networks\n",
    "- Recurrent Neural Networks (RNN) are a type of neural network with loops\n",
    "- Hidden layer outputs are now a function of current input x<sub>t</sub> and one or more previous inputs x<sub>t-1</sub>\n",
    "    - great solution for problems involving sequences (speech, text, music, stocks, etc)\n",
    "\n",
    "#### Long Short-Term Memory (LSTM) Networks\n",
    "- [LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) are a type of RNN that is capable of learning longer distance dependencies than traditional RNNs\n",
    "- LSTM units contain gates which allow it to remember and forget previous inputs\n",
    "    - even better for NLP problems where we are working with sequence of words, because words have long distance dependencies\n",
    "        \n",
    "#### Goal\n",
    "- to use a Long Short-Term Memory (LSTM) Network to predict if a movie review is positive or negative with high accuracy\n",
    "\n",
    "----\n",
    "\n",
    "All credit goes to [Adit Deshpande](https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow), the foundation of this project was laid with his tutorial.\n",
    "\n",
    "\n",
    "Lets get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- must convert movie review text to scalar input for neural network\n",
    "- word vectors are able to represent context, meaning, and semantics\n",
    "    - similar words or similar parts of speech will have vector definitions with high similarity\n",
    "- many advantages over traditional bag of words models\n",
    "- to save time we will use word vectors provided by [GloVe](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation)\n",
    "- alternatively, one could train model to create word vectors, TensorFlow offers a [tutorial on Word2Vec](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loaded total words = ', 400000)\n",
      "('Word Vector shape = ', (400000, 50))\n"
     ]
    }
   ],
   "source": [
    "# load lexicon word list and decode\n",
    "wordsList = np.load('lexicon/wordsList.npy')\n",
    "wordsList = list(map(lambda word: word.decode('UTF-8'), wordsList))\n",
    "\n",
    "# load word vector representation for each word in lexicon\n",
    "wordVectors = np.load('lexicon/wordVectors.npy')\n",
    "\n",
    "print ('Loaded total words = ', len(wordsList))\n",
    "print('Word Vector shape = ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that GloVe has provided us 400,000 words in their word vector list\n",
    "- also note that each word vector has dimensionality of 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the movie data set is split into 2 directories, positive and negative\n",
    "- in each directory, there is 1 file per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveDir = 'data/positiveReviews/'\n",
    "negativeDir = 'data/negativeReviews/'\n",
    "\n",
    "# create list of positive files\n",
    "positiveFiles = []\n",
    "for file in listdir(positiveDir):\n",
    "    if isfile(join(positiveDir, file)):\n",
    "        positiveFiles.append(join(positiveDir, file))\n",
    "\n",
    "# create list of negative files\n",
    "negativeFiles = []\n",
    "for file in listdir(negativeDir):\n",
    "    if isfile(join(negativeDir, file)):\n",
    "        negativeFiles.append(join(negativeDir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "- first we will view file count, word count, and avg words per file\n",
    "- then we use a histogram to examine movie review word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('File Count: ', 25000)\n",
      "('Positive Files: ', 12500)\n",
      "('Negative Files: ', 12500)\n",
      "('Word Count: ', 5844464)\n",
      "('Avg Words per File: ', 233)\n"
     ]
    }
   ],
   "source": [
    "# count words to get better idea of our data\n",
    "\n",
    "wordCount = []\n",
    "\n",
    "for file in positiveFiles:\n",
    "    with open(file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        wordCount.append(counter)       \n",
    "\n",
    "for file in negativeFiles:\n",
    "    with open(file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        wordCount.append(counter)  \n",
    "\n",
    "fileCount = len(wordCount)\n",
    "\n",
    "print('File Count: ', fileCount)\n",
    "print('Positive Files: ', len(positiveFiles))\n",
    "print('Negative Files: ', len(negativeFiles))\n",
    "print('Word Count: ', sum(wordCount))\n",
    "print('Avg Words per File: ', sum(wordCount)/len(wordCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(x=wordCount, bins=100)\n",
    "plt.title('Sequence Length Histogram')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.axis([0, 1050, 0, 4000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- looks like most words have length 50 - 250 words\n",
    "- we will define a maximum sequence length of 250 words\n",
    "    - later we will try a maximum sequence length of 400 words to see if it makes any improvements\n",
    "    - potentially, longer sequence length will provides more information at cost of time and space complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining movie review dimensionality\n",
    "maxSeqLength = 250\n",
    "\n",
    "# word vector length as provided by GloVe\n",
    "wordVecLength = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n"
     ]
    }
   ],
   "source": [
    "# let's preview a movie review\n",
    "fname = positiveFiles[0]\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like we will need to remove punctuation and apply case folding\n",
    "# other nlp processing tasks can be applied here during tuning\n",
    "\n",
    "# helper function for cleaning text\n",
    "# case folding and removing special characters\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def cleanText(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Movie Reviews to Word Vectors\n",
    "- convert each text file (review) to a word indices vector, where the indice corresponds to the indices in the lexicon provided by GloVe\n",
    "- this review - word indices matrix will allow us to look up embeddings when it comes time to train our model \n",
    "- save this mapping once complete, takes about 20 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commenting this out, no need to run again, takes 20 minutes total\n",
    "\n",
    "# %%time\n",
    "\n",
    "# # we need to convert each file (individual review) to word indice vector\n",
    "# # resulting file representation will be vector of indices, each indice corresponding to original word\n",
    "\n",
    "# # create empty word indices matrix\n",
    "# reviewWordIndices = np.zeros((fileCount, maxSeqLength), dtype='int32')\n",
    "\n",
    "# fileCounter = 0\n",
    "\n",
    "# # process positive files\n",
    "# for file in positiveFiles:\n",
    "#     with open(file, \"r\", encoding='utf-8') as f:\n",
    "#         idx = 0\n",
    "#         # clean and tokenize\n",
    "#         line = f.readline()\n",
    "#         line = cleanText(line)\n",
    "#         line = line.split()\n",
    "#         # store index for each word\n",
    "#         for word in line:\n",
    "#             try:\n",
    "#                 reviewWordIndices[fileCounter][idx] = wordsList.index(word)\n",
    "#             except ValueError:\n",
    "#                 reviewWordIndices[fileCounter][idx] = 399999 # Unknown word vector\n",
    "#             idx += 1\n",
    "#             if idx >= maxSeqLength:\n",
    "#                 break\n",
    "#         if fileCounter % 100 is 0:\n",
    "#             print('Completed positive file ', fileCounter)\n",
    "#         fileCounter += 1\n",
    "\n",
    "        \n",
    "# print('Positive files complete!')\n",
    "\n",
    "\n",
    "# # repeat same process for negative files\n",
    "# for file in negativeFiles:\n",
    "#     with open(file, \"r\", encoding='utf-8') as f:\n",
    "#         idx = 0\n",
    "#         # clean and tokenize\n",
    "#         line = f.readline()\n",
    "#         line = cleanText(line)\n",
    "#         line = line.split()\n",
    "#         # store index for each word\n",
    "#         for word in line:\n",
    "#             try:\n",
    "#                 reviewWordIndices[fileCounter][idx] = wordsList.index(word)\n",
    "#             except ValueError:\n",
    "#                 reviewWordIndices[fileCounter][idx] = 399999 # Unknown word vector\n",
    "#             idx += 1\n",
    "#             if idx >= maxSeqLength:\n",
    "#                 break\n",
    "#         if fileCounter % 100 is 0:\n",
    "#             print('Completed negative file ', fileCounter)\n",
    "#         fileCounter += 1\n",
    "\n",
    "        \n",
    "# print('Negative files complete!')\n",
    "\n",
    "\n",
    "# # Save our matrix to file\n",
    "# np.save('data/wordIndicesMatrix', reviewWordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  9520,   5988,   1005,   2703,     17,   1613,   1919,     34,\n",
       "         4705,     21,  19664,  26435,    168,      3,  17099,  35115,\n",
       "          586,     19,      7,   1922,     34,   3779,  11178,   1423,\n",
       "           17,      7,  71488,   6107,     38,     14,   7866,      4,\n",
       "          642,     10,     71,   3181,  18018,     21,  11863, 201534,\n",
       "         9815,      4,   5613, 201534,   3468,     17,  26435,  12246,\n",
       "           32,    191,    143,   2053, 201534,   5030,    403,     14,\n",
       "         2615,  11280, 206163,     14,  12387,     19, 149242,  12138,\n",
       "           34,     61,  26435,   2478,     75,      7,  24674,   4430,\n",
       "         5624,  44695,  10338,     21,      7,   5651,   4314,     13,\n",
       "       201534,    220,   1583,    654,    588,    465,   5015, 201534,\n",
       "         3545,    144,      7,  59406,  16313,  17725,  16375,      5,\n",
       "        43146,   1334,  16239,    679,   8575, 393493,     32,      7,\n",
       "        45616,    530,      5,  12725,  43488,     14,    353,   2905,\n",
       "           19,      7,  32358,    142,   6634, 201534,   1005,     14,\n",
       "          573,      7, 399999,      3, 399999,   2232,      5, 201534,\n",
       "       399999,    102,      7,   3398,    243,     13, 201534,  22533,\n",
       "           21,   6806, 399999, 201534,  17160,     14, 104926,  31809,\n",
       "          549,      3,  15637,    845,    138,    143,     21,    369,\n",
       "          785,   1364,     38,  18987,      7,  52332,  31318,   1945,\n",
       "           17,   7404,    421,     25,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our word indices matrix into memory\n",
    "wordIndicesMatrix = np.load('data/wordIndicesMatrix.npy')\n",
    "\n",
    "# inspect shape of our document - wordIndice matrix\n",
    "# Expecting:\n",
    "# 25K movie reviews\n",
    "# maximum review length of 250\n",
    "print wordIndicesMatrix.shape\n",
    "\n",
    "# preview a movie review representation\n",
    "wordIndicesMatrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Into Training / Test Sets\n",
    "- we are defining methods that will fetch batches of data during training and testing\n",
    "- there is room for improvement here, we are not shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions for getting training and test batches\n",
    "\n",
    "# we are using holdout of 8% for test data (2K reviews)\n",
    "\n",
    "# Training Set\n",
    "# 1 < positive training instances < 11499\n",
    "# 13499 < negative training instances < 24999\n",
    "# training set is 23,000 movie reviews\n",
    "# 11,500 positive, 11,500 negative\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # alternate between positive / negative instances\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            # positive samples\n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            # negative samples\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = wordIndicesMatrix[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "\n",
    "# Test Set\n",
    "# we're getting test instances from indices\n",
    "# 11500 < test data < 13500\n",
    "# giving us 2K test instances, half positive and half negative\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = wordIndicesMatrix[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TensorFlow Graph for LSTM Network\n",
    "\n",
    "### Defining LSTM Network Parameters\n",
    "- 64 and 128 LSTM units were tested\n",
    "- increasing LSTM units should allow model to learn more complex patterns at the cost of increased resource (time and space) requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN and trianing params\n",
    "batchSize = 50\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "epochs = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Graph\n",
    "- define the TensorFlow graph using TensorFlow Python API\n",
    "- organize like types and operations with name spaces to keep graph visualization clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# placeholder for input features\n",
    "with tf.name_scope('Features'):\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength], name=\"Features\")\n",
    "\n",
    "    \n",
    "# placeholder for input labels\n",
    "with tf.name_scope(\"Labels\"):\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses], name=\"Labels\")\n",
    "\n",
    "        \n",
    "# placeholder for word vectors previously loaded from GloVe\n",
    "with tf.name_scope(\"WordVectors\"):\n",
    "    word_vectors = tf.placeholder(tf.float32, [400000, 50], name=\"WordVectors\")\n",
    "    \n",
    "    \n",
    "# operation - lookup word vector embedding for input features\n",
    "with tf.name_scope(\"EmbeddingLookup\"):\n",
    "    data = tf.Variable(tf.zeros([batchSize, maxSeqLength, wordVecLength]),dtype=tf.float32, name=\"Embedding\")\n",
    "    data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "\n",
    "    \n",
    "# variable - weights initialized as random values from a truncated normal distribution\n",
    "with tf.name_scope(\"Weights\"):\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]), name=\"Weights\")\n",
    "\n",
    "    \n",
    "# variable - Bias initialized to 0.1\n",
    "with tf.name_scope(\"Bias\"):\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]), name=\"Bias\")\n",
    "\n",
    "    \n",
    "# operations related to prediction\n",
    "# grabs output from RNN, multiplies by weights and adds bias\n",
    "with tf.name_scope(\"RNN\"):\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels)\n",
    "    \n",
    "    \n",
    "# operation - accuracy calculation\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "    test_acc_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "    \n",
    "    \n",
    "# operation - loss calculation\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(softmax)\n",
    "    tf.summary.scalar('Loss', loss)\n",
    "\n",
    "    \n",
    "# define optimization strategy to be executed during training\n",
    "with tf.name_scope(\"Optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Session and Execute TensorFlow Graph\n",
    "- initialize new session\n",
    "- create FileWriter instances to record results to Tensorboard\n",
    "- for each epoch: perform mini-batch optimization\n",
    "    - RNN is fed a sample of training data of batchSize\n",
    "    - error is calculated and model updated after each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-5000\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "saved to models/pretrained_lstm.ckpt-15000\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "saved to models/pretrained_lstm.ckpt-25000\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "saved to models/pretrained_lstm.ckpt-35000\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "saved to models/pretrained_lstm.ckpt-45000\n",
      "CPU times: user 4h 1min 20s, sys: 53min 9s, total: 4h 54min 30s\n",
      "Wall time: 1h 29min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Init new session and saver to store checkpoints\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# create summary writers to store graph and chart metrics\n",
    "merged = tf.summary.merge_all()\n",
    "trainLogDir = \"tensorboard/graph_e50K_b50_lstm64_lr001_train\"\n",
    "writer = tf.summary.FileWriter(trainLogDir, sess.graph)\n",
    "\n",
    "\n",
    "# we create separate summary writer for test data evaluations\n",
    "# this is workaround to let us view train vs test accuracy on same plot\n",
    "testLogDir = \"tmpboard/graph_e50K_b50_lstm64_001)test\"\n",
    "testWriter = tf.summary.FileWriter(testLogDir)\n",
    "\n",
    "\n",
    "tenPercent = int(epochs * 0.1)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "   # Train Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {word_vectors: wordVectors, input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "\n",
    "   # Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {word_vectors: wordVectors, input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "        \n",
    "        testBatch, testLabels = getTestBatch()\n",
    "        test_summary_str = sess.run(test_acc_summary,{word_vectors: wordVectors, input_data: testBatch, labels: testLabels})\n",
    "        \n",
    "        testWriter.add_summary(test_summary_str, i)\n",
    "\n",
    "        \n",
    "   # Save the network every 10% completed\n",
    "   if (i % tenPercent == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "        \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring A Saved Model\n",
    "- good practice for long problems is to store the network at checkpoints during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-45\n"
     ]
    }
   ],
   "source": [
    "# restore our model and check accuracy against test data\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 62.00000047683716)\n",
      "('Accuracy for this batch:', 66.00000262260437)\n",
      "('Accuracy for this batch:', 47.999998927116394)\n",
      "('Accuracy for this batch:', 47.999998927116394)\n",
      "('Accuracy for this batch:', 57.999998331069946)\n",
      "('Accuracy for this batch:', 54.00000214576721)\n",
      "('Accuracy for this batch:', 54.00000214576721)\n",
      "('Accuracy for this batch:', 36.000001430511475)\n",
      "('Accuracy for this batch:', 54.00000214576721)\n",
      "('Accuracy for this batch:', 47.999998927116394)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {word_vectors: wordVectors, input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization With TensorBoard\n",
    "- start up tensorboard server to view computation graph and training/testing evaluation metrics\n",
    "- we've named our tensorboard log directory 'tensorboard'\n",
    "```\n",
    "$ tensorboard --logdir tensorboard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
